{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "import implicit\n",
    "from implicit import evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Self-written classes\n",
    "from Threshold_Filtering import Threshold_filter\n",
    "threshold_filter = Threshold_filter()\n",
    "\n",
    "from Scoring_Approaches import Scoring_Approaches\n",
    "preference_scoring = Scoring_Approaches()\n",
    "\n",
    "from Baseline_recommenders import Popularity_recommender, Random_recommender\n",
    "pop_recommender = Popularity_recommender()\n",
    "rand_recommender = Random_recommender()\n",
    "\n",
    "from Evaluation_Metrics import Evaluation_Metrics\n",
    "eval =  Evaluation_Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the raw data\n",
    "df_raw = pd.read_csv('Interaction_Data.csv')\n",
    "\n",
    "# Proceeding with only columns that are required for offline eval\n",
    "df = df_raw.filter([\"user_id\", \"item_id\", \"click\", \"Impressions\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dataframe in which all users have clicked at least 3 different ads, and each ad has been clicked by at least 10 different users\n",
    "df = threshold_filter.both_total_clicks(df.copy(),3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the inferences from all scoring approaches\n",
    "threshold = 4\n",
    "df = preference_scoring.all_approaches(threshold, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train+test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df[df['click'] > 0]\n",
    "df_neg = df[df['click'] == 0]\n",
    "\n",
    "X = df_pos.copy()\n",
    "y = df_pos[\"user_id\"]\n",
    "\n",
    "#stratify so each user is present in both dataframes, and random_state for reproducibility\n",
    "pos_train_df, test_df = train_test_split(X, test_size=0.30, stratify=y,random_state=42)\n",
    "\n",
    "X_neg = df_neg.copy()\n",
    "y_neg = df_neg[\"user_id\"]\n",
    "neg_train, neg_test = train_test_split(X_neg, test_size=0.30, stratify=y_neg,random_state=42) \n",
    "\n",
    "\n",
    "# Merging the negative interactions into the postive training set. \n",
    "train_df = pd.concat([pos_train_df, neg_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_traintest_matrix(train_df, test_df, col_name):\n",
    "    \"\"\"\n",
    "    Create sparse matrices from the given train and test dataframes, with the given column name as the data.\n",
    "\n",
    "    Parameters:\n",
    "    train_df (pandas.DataFrame): The training dataframe containing 'user_id', 'item_id', and 'col_name' columns.\n",
    "    test_df (pandas.DataFrame): The test dataframe containing 'user_id', 'item_id', and 'col_name' columns.\n",
    "    col_name (str): The name of the column to be used for creating the sparse matrices.\n",
    "\n",
    "    Returns:\n",
    "    train (scipy.sparse.csr_matrix)\n",
    "    test (scipy.sparse.csr_matrix)\n",
    "    \"\"\"\n",
    "    train_df = train_df.copy().filter(['user_id', 'item_id', col_name])\n",
    "    test_df = test_df.copy().filter(['user_id', 'item_id', col_name])\n",
    "    train = csr_matrix((train_df[col_name], (train_df[\"user_id\"], train_df[\"item_id\"])))\n",
    "    test = csr_matrix((test_df[col_name], (test_df[\"user_id\"], test_df[\"item_id\"])))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Making different matrices for each of the different scoring approaches \n",
    "in order to compare results at the end\n",
    "'''\n",
    "train_binary, test_binary = sparse_traintest_matrix(train_df, test_df, \"binary\")\n",
    "train_CTR, test_CTR = sparse_traintest_matrix(train_df, test_df, \"CTR\")\n",
    "train_N, test_N = sparse_traintest_matrix(train_df, test_df, \"N\")\n",
    "train_SqrtN, test_SqrtN = sparse_traintest_matrix(train_df, test_df, \"SqrtN\")\n",
    "train_IPN, test_IPN = sparse_traintest_matrix(train_df, test_df, \"IPN\")\n",
    "train_click, test_click = sparse_traintest_matrix(train_df, test_df, \"click\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traintest_matrices = {'click':(train_click,test_click),\n",
    "                      'binary':(train_binary,test_binary),\n",
    "                      'CTR':(train_CTR,test_CTR),\n",
    "                      'N':(train_N,test_N),\n",
    "                      'SqrtN':(train_SqrtN,test_SqrtN),\n",
    "                      'IPN':(train_IPN,test_IPN)                                   \n",
    "                      }\n",
    "\n",
    "scoring_keys = traintest_matrices.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and training the matrix factorization models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperparameters were found through gridsearch\n",
    "\n",
    "ALS_params = {'click': {'factors': 5, 'regularization': 0.001, 'iterations': 5}, \n",
    "              'binary': {'factors': 5, 'regularization': 0.001, 'iterations': 5},\n",
    "              'CTR': {'factors': 5, 'regularization': 0.001, 'iterations': 5}, \n",
    "              'N': {'factors': 5, 'regularization': 0.01, 'iterations': 5},\n",
    "              'SqrtN': {'factors': 5, 'regularization': 0.1, 'iterations': 5}, \n",
    "              'IPN': {'factors': 5, 'regularization': 0.1, 'iterations': 10}}\n",
    "\n",
    "LMF_params = {'click': {'factors': 60, 'regularization': 6.0, 'iterations': 60, 'learning_rate': 1.0}, \n",
    "              'binary': {'factors': 40, 'regularization': 6.0, 'iterations': 50, 'learning_rate': 1.0},\n",
    "              'CTR': {'factors': 60, 'regularization': 0.6, 'iterations': 100, 'learning_rate': 1.0}, \n",
    "              'N': {'factors': 5, 'regularization': 0.6, 'iterations': 80, 'learning_rate': 1.0}, \n",
    "              'SqrtN': {'factors': 5, 'regularization': 0.6, 'iterations': 80, 'learning_rate': 1.0}, \n",
    "              'IPN': {'factors': 5, 'regularization': 6.0, 'iterations': 80, 'learning_rate': 1.0}}\n",
    "\n",
    "# Best hyperparams for BPR were the default params of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 24.42it/s]\n",
      "100%|██████████| 60/60 [00:08<00:00,  6.95it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 25.12it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 12.62it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 26.51it/s]\n",
      "100%|██████████| 100/100 [00:14<00:00,  7.03it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 26.42it/s]\n",
      "100%|██████████| 80/80 [00:01<00:00, 44.59it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 25.96it/s]\n",
      "100%|██████████| 80/80 [00:01<00:00, 44.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 26.34it/s]\n",
      "100%|██████████| 80/80 [00:01<00:00, 49.14it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 14.15it/s, train_auc=88.52%, skipped=34.30%]\n"
     ]
    }
   ],
   "source": [
    "lmf_models = {}\n",
    "als_models = {}\n",
    "bpr_models = {}\n",
    "for i in scoring_keys:\n",
    "    als_models[i] = implicit.als.AlternatingLeastSquares(factors=ALS_params[i]['factors'], regularization=ALS_params[i]['regularization'], iterations=ALS_params[i]['iterations'],random_state=42)\n",
    "    als_models[i].fit(traintest_matrices[i][0])\n",
    "\n",
    "    lmf_models[i] = implicit.lmf.LogisticMatrixFactorization(factors=LMF_params[i]['factors'], regularization=LMF_params[i]['regularization'], iterations=LMF_params[i]['iterations'], learning_rate=LMF_params[i]['learning_rate'],random_state=42)\n",
    "    lmf_models[i].fit(traintest_matrices[i][0])\n",
    "\n",
    "bpr_models['binary'] = implicit.bpr.BayesianPersonalizedRanking(random_state=42)\n",
    "bpr_models['binary'].fit(traintest_matrices['binary'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making list of the unique users in the data set, and a list of the items they clicked from the test set\n",
    "\n",
    "pos_train = train_df[train_df['click'] > 0]\n",
    "pos_test = test_df.copy()\n",
    "\n",
    "unique_users = df['user_id'].unique()\n",
    "user_test_clicks = []\n",
    "\n",
    "for user in unique_users:\n",
    "    clicks = pos_test[pos_test[\"user_id\"] == user][\"item_id\"].tolist()\n",
    "    user_test_clicks.append(clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Both functions below return a list of lists containing \n",
    "each unique users N number of recommendations from the given model\n",
    "'''\n",
    "\n",
    "def model_recommendations(model, train, N=10, filter=True):\n",
    "    user_recommendations = []\n",
    "    for user in unique_users:\n",
    "        if type(train) is csr_matrix:\n",
    "            recs,scores = model.recommend(user, train[user], N=N, filter_already_liked_items=filter)\n",
    "        else:\n",
    "            recs,scores = model.recommend(user, N=N)\n",
    "        user_recommendations.append(recs)\n",
    "    \n",
    "    return user_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function calculates and prints the performance metrics (Precision, Recall,MAP, AUC, NDCG) from the given model trained on each scoring approach.\n",
    "\n",
    "Parameters:\n",
    "- model_name (str): The name of the model.\n",
    "- model_dict (dict): A dictionary containing the different preference score names as keys and the model that is trained on these scores as values.\n",
    "\n",
    "Returns:\n",
    "None\n",
    "\"\"\"\n",
    "\n",
    "def model_performance_metrics(model_name,model_dict, k=10):\n",
    "    r = 4 # Rounding to 4 decimal places\n",
    "    print(model_name,f'\\n{\"-\"*75}\\n     {\"\".ljust(9)}| {\"Precision\".ljust(10)}| {\"Recall\".ljust(10)}| {\"MAP\".ljust(10)}| {\"AUC\".ljust(10)}| {\"NDCG\".ljust(10)} \\n{\"-\"*75}')\n",
    "\n",
    "    for key in model_dict:\n",
    "        recs = model_recommendations(model_dict[key], traintest_matrices[key][0], N=k)\n",
    "        traintest = traintest_matrices[key]\n",
    "        resDict = implicit.evaluation.ranking_metrics_at_k(model_dict[key],traintest[0], traintest[1], show_progress=False, K=k)\n",
    "        p_,r_ = eval.precision_recall(user_test_clicks, recs)\n",
    "        mean_avg_p = eval.mean_average_precision(user_test_clicks, recs)\n",
    "        ndcg = eval.ndcg(user_test_clicks, recs)\n",
    "\n",
    "        print(f'    {key.ljust(10)}| {str(round(p_,r)).ljust(10)}| {str(round(r_,r)).ljust(10)}| {str(round(mean_avg_p,r)).ljust(10)}| {str(round(resDict[\"auc\"],r)).ljust(10)}| {str(round(ndcg,r)).ljust(10)}')\n",
    "    print()\n",
    "\n",
    "'''\n",
    "Prints the scores across all performance metrics for the baseline recommenders\n",
    "'''\n",
    "def baseline_performance_metrics(model_dict, k=10):\n",
    "    r = 4\n",
    "    print('Baselines',f'\\n{\"-\"*75}\\n     {\"\".ljust(9)}| {\"Precision\".ljust(10)}| {\"Recall\".ljust(10)}| {\"MAP\".ljust(10)}| {\"AUC\".ljust(10)}| {\"NDCG\".ljust(10)} \\n{\"-\"*75}')\n",
    "\n",
    "    for key in model_dict:\n",
    "        recs = model_recommendations(model_dict[key], train_df, N=k)\n",
    "        p_,r_ = eval.precision_recall(user_test_clicks, recs)\n",
    "        mean_avg_p = eval.mean_average_precision(user_test_clicks, recs)\n",
    "        ndcg = eval.ndcg(user_test_clicks, recs)\n",
    "\n",
    "        # Using only my performance metrics\n",
    "        print(f'    {key.ljust(10)}| {str(round(p_,r)).ljust(10)}| {str(round(r_,r)).ljust(10)}| {str(round(mean_avg_p,r)).ljust(10)}| {\"-\".ljust(10)}| {str(round(ndcg,r)).ljust(10)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALS Evaluation \n",
      "---------------------------------------------------------------------------\n",
      "              | Precision | Recall    | MAP       | AUC       | NDCG       \n",
      "---------------------------------------------------------------------------\n",
      "    click     | 0.0742    | 0.2625    | 0.1573    | 0.6256    | 0.1911    \n",
      "    binary    | 0.074     | 0.2635    | 0.155     | 0.6261    | 0.189     \n",
      "    CTR       | 0.0548    | 0.1915    | 0.1129    | 0.5899    | 0.1411    \n",
      "    N         | 0.0693    | 0.2415    | 0.1445    | 0.6151    | 0.1793    \n",
      "    SqrtN     | 0.0695    | 0.2437    | 0.1457    | 0.6161    | 0.179     \n",
      "    IPN       | 0.0771    | 0.2688    | 0.171     | 0.6289    | 0.2065    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performance_metrics('ALS Evaluation', als_models, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMF Evaluation \n",
      "---------------------------------------------------------------------------\n",
      "              | Precision | Recall    | MAP       | AUC       | NDCG       \n",
      "---------------------------------------------------------------------------\n",
      "    click     | 0.0722    | 0.2316    | 0.1372    | 0.6101    | 0.1868    \n",
      "    binary    | 0.0679    | 0.2156    | 0.1235    | 0.602     | 0.1734    \n",
      "    CTR       | 0.0634    | 0.2058    | 0.1129    | 0.597     | 0.1573    \n",
      "    N         | 0.0738    | 0.2411    | 0.1484    | 0.6149    | 0.1956    \n",
      "    SqrtN     | 0.0728    | 0.2423    | 0.1549    | 0.6155    | 0.1964    \n",
      "    IPN       | 0.0527    | 0.1763    | 0.0985    | 0.5821    | 0.1341    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performance_metrics('LMF Evaluation', lmf_models,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPR Evaluation \n",
      "---------------------------------------------------------------------------\n",
      "              | Precision | Recall    | MAP       | AUC       | NDCG       \n",
      "---------------------------------------------------------------------------\n",
      "    binary    | 0.0524    | 0.1997    | 0.1033    | 0.5938    | 0.1267    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_performance_metrics('BPR Evaluation', bpr_models, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_recommender.fit(train_df)\n",
    "rand_recommender.fit(train_df)\n",
    "baseline_dict = {'Popularity':pop_recommender,'Random':rand_recommender}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baselines \n",
      "---------------------------------------------------------------------------\n",
      "              | Precision | Recall    | MAP       | AUC       | NDCG       \n",
      "---------------------------------------------------------------------------\n",
      "    Popularity| 0.0703    | 0.226     | 0.1421    | -         | 0.1887    \n",
      "    Random    | 0.0065    | 0.0202    | 0.0093    | -         | 0.0147    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_performance_metrics(baseline_dict, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
